# Descriptor Research Project

A comprehensive computer vision framework for comparing and evaluating image descriptors (SIFT, RGBSIFT, HoNC, etc.) using domain size pooling, stacking, and other advanced processing techniques.

## Table of Contents
- [Quick Start](#quick-start)
- [Installation Methods](#installation-methods)
- [Development Environments](#development-environments)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Troubleshooting](#troubleshooting)

## Quick Start

### Option 1: Docker (Recommended - Works Everywhere)
```bash
git clone <your-repository-url>
cd descriptor-compare
python3 setup.py  # Downloads HPatches dataset
docker build -t descriptor-research:dev --target development .
docker-compose -f docker-compose.dev.yml up -d
docker-compose -f docker-compose.dev.yml exec descriptor-dev bash
cd /workspace/build-docker && cmake .. && make -j$(nproc)
./descriptor_compare
```

### Option 2: Native Linux (Manjaro/Arch)
```bash
git clone <your-repository-url>
cd descriptor-compare
sudo pacman -S base-devel cmake opencv boost tbb sqlite
python3 setup.py  # Downloads HPatches dataset
mkdir build && cd build && cmake .. -DBUILD_DATABASE=ON && make -j$(nproc)
./descriptor_compare
```

### Option 3: Windows (Docker - Easiest)
```powershell
git clone <your-repository-url>
cd descriptor-compare
python setup.py  # Downloads HPatches dataset
docker build -t descriptor-research:dev --target development .
docker run -it --rm -v ${PWD}:/workspace descriptor-research:dev bash
# Inside container: cd /workspace/build-docker && cmake .. && make -j$(nproc) && ./descriptor_compare
```

## Installation Methods

### Docker Installation (Recommended for Reproducibility)

#### Prerequisites
```bash
# Install Docker and Docker Compose

# Linux (Manjaro/Arch)
sudo pacman -S docker docker-compose

# Linux (Ubuntu/Debian)
sudo apt install docker.io docker-compose

# macOS
brew install docker docker-compose
# OR install Docker Desktop from https://www.docker.com/products/docker-desktop/

# Windows
# Install Docker Desktop from https://www.docker.com/products/docker-desktop/
# Enable WSL2 integration if using WSL2

# Add user to docker group (Linux only)
sudo usermod -aG docker $USER
sudo systemctl start docker
sudo systemctl enable docker
# Log out and back in for group changes
```

#### Build and Run
```bash
# Clone repository
git clone <your-repository-url>
cd descriptor-compare

# Download dataset (required)
python3 setup.py

# Build Docker environment
docker build -t descriptor-research:dev --target development .

# Start development environment
docker-compose -f docker-compose.dev.yml up -d

# Enter development container
docker-compose -f docker-compose.dev.yml exec descriptor-dev bash

# Build project inside container
cd /workspace
mkdir build-docker && cd build-docker
cmake .. -DUSE_SYSTEM_PACKAGES=ON -DUSE_CONAN=OFF -DBUILD_DATABASE=ON -DBUILD_DATABASE=ON
make -j$(nproc)

# Run experiments (database-enabled)
./descriptor_compare
```

#### Docker Environment Features
- âœ… **Reproducible builds** across all systems
- âœ… **Pre-configured OpenCV** with SIFT support
- âœ… **All dependencies included**
- âœ… **X11 forwarding** for GUI debugging
- âœ… **Volume mounting** for live code editing

### Native Installation

#### Manjaro/Arch Linux
```bash
# Install system dependencies
sudo pacman -S base-devel cmake git python python-pip
sudo pacman -S opencv boost tbb intel-tbb sqlite

# Install OpenCV contrib (for SIFT support)
yay -S opencv-contrib  # or paru -S opencv-contrib

# Clone and build
git clone <your-repository-url>
cd descriptor-compare
python3 setup.py  # Download HPatches dataset
mkdir build && cd build
cmake .. -DUSE_SYSTEM_PACKAGES=ON -DUSE_CONAN=OFF -DBUILD_DATABASE=ON
make -j$(nproc)
./descriptor_compare
```

#### Ubuntu/Debian
```bash
# Install system dependencies
sudo apt update
sudo apt install build-essential cmake git python3 python3-pip
sudo apt install libopencv-dev libopencv-contrib-dev
sudo apt install libboost-all-dev libtbb-dev libsqlite3-dev

# Clone and build
git clone <your-repository-url>
cd descriptor-compare
python3 setup.py  # Download HPatches dataset
mkdir build && cd build
cmake .. -DUSE_SYSTEM_PACKAGES=ON -DUSE_CONAN=OFF -DBUILD_DATABASE=ON
make -j$(nproc)
./descriptor_compare
```

#### macOS
```bash
# Install Homebrew if not already installed
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Install dependencies
brew install cmake opencv boost tbb python3 sqlite

# Clone and build
git clone <your-repository-url>
cd descriptor-compare
python3 setup.py  # Download HPatches dataset
mkdir build && cd build
cmake .. -DUSE_SYSTEM_PACKAGES=ON -DUSE_CONAN=OFF -DBUILD_DATABASE=ON
make -j$(nproc)
./descriptor_compare
```

#### Windows

##### Option 1: Docker (Recommended)
```powershell
# Install Docker Desktop from https://www.docker.com/products/docker-desktop/

# Clone repository
git clone <your-repository-url>
cd descriptor-compare

# Download dataset
python setup.py

# Build and run in Docker
docker build -t descriptor-research:dev --target development .
docker run -it --rm -v ${PWD}:/workspace descriptor-research:dev bash

# Inside container:
cd /workspace
mkdir build-docker && cd build-docker
cmake .. -DUSE_SYSTEM_PACKAGES=ON -DUSE_CONAN=OFF -DBUILD_DATABASE=ON
make -j$(nproc)
./descriptor_compare
```

##### Option 2: Native Windows (Advanced)
```powershell
# Install Visual Studio 2022 with C++ workload
# Install vcpkg package manager
git clone https://github.com/Microsoft/vcpkg.git
cd vcpkg
.\bootstrap-vcpkg.bat
.\vcpkg integrate install

# Install dependencies via vcpkg
.\vcpkg install opencv[contrib]:x64-windows
.\vcpkg install boost:x64-windows
.\vcpkg install tbb:x64-windows

# Clone and build project
git clone <your-repository-url>
cd descriptor-compare
python setup.py  # Download HPatches dataset

# Build with Visual Studio
mkdir build && cd build
cmake .. -DCMAKE_TOOLCHAIN_FILE=C:/path/to/vcpkg/scripts/buildsystems/vcpkg.cmake -DUSE_SYSTEM_PACKAGES=ON
cmake --build . --config Release
.\Release\descriptor_compare.exe
```

##### Option 3: WSL2 (Linux Subsystem)
```bash
# Install WSL2 and Ubuntu from Microsoft Store
# Follow Ubuntu installation instructions inside WSL2:

sudo apt update
sudo apt install build-essential cmake git python3 python3-pip
sudo apt install libopencv-dev libopencv-contrib-dev
sudo apt install libboost-all-dev libtbb-dev libsqlite3-dev

git clone <your-repository-url>
cd descriptor-compare
python3 setup.py  # Download HPatches dataset
mkdir build && cd build
cmake .. -DUSE_SYSTEM_PACKAGES=ON -DUSE_CONAN=OFF -DBUILD_DATABASE=ON
make -j$(nproc)
./descriptor_compare
```

## Development Environments

### CLion Integration (Recommended for Development)

#### Multi-Environment Setup
1. **Open project in CLion**
2. **Configure Docker Toolchain**:
   - Settings â†’ Build â†’ Toolchains
   - Add Docker toolchain: `Docker Ubuntu 22.04`
   - Image: `descriptor-research:dev`

3. **Create CMake Profiles**:
   - **Native-Debug**: Fast local development
   - **Docker-Debug**: Reproducible container development
   - **Native-Release**: Performance testing
   - **Docker-Release**: Reproducible performance testing

#### CLion CMake Profiles Configuration

**Native-Debug:**
```
Build type: Debug
CMake options: -DUSE_SYSTEM_PACKAGES=ON -DUSE_CONAN=OFF
Build directory: cmake-build-debug
```

**Docker-Debug:**
```
Build type: Debug
Toolchain: Docker Ubuntu 22.04
CMake options: -DUSE_SYSTEM_PACKAGES=ON -DUSE_CONAN=OFF
Build directory: cmake-build-docker-debug
```

#### Switching Environments
- **Select configuration** from CLion toolbar dropdown
- **Build/Debug/Run** seamlessly in either environment
- **Native**: Full speed, all debugging features
- **Docker**: Reproducible, shareable environment

### Command Line Development

#### Docker Development Workflow
```bash
# Start development environment
docker-compose -f docker-compose.dev.yml up -d

# Develop with live code mounting
docker-compose -f docker-compose.dev.yml exec descriptor-dev bash

# Build and test
cd /workspace/build-docker
make -j$(nproc)
./descriptor_compare

# Stop environment
docker-compose -f docker-compose.dev.yml down
```

#### Native Development Workflow
```bash
# Standard CMake workflow
cd build
make -j$(nproc)
./descriptor_compare

# Or rebuild from scratch
cd .. && rm -rf build && mkdir build && cd build
cmake .. && make -j$(nproc)
```

## Usage

### Running Experiments

#### NEW: CLI + YAML Configuration (Recommended)
```bash
cd build
./experiment_runner ../config/experiments/sift_baseline.yaml
./experiment_runner ../config/experiments/rgbsift_comparison.yaml

# The CLI experiment runner will:
# 1. Load YAML experiment configuration
# 2. Extract descriptors using YAML-defined methods
# 3. Evaluate precision using homography validation  
# 4. Save results to results/experiment_name/ directory
```

#### Legacy: Basic Usage
```bash
# Run with hardcoded configuration
./descriptor_compare

# The program will:
# 1. Load HPatches dataset from data/
# 2. Extract descriptors using hardcoded methods
# 3. Evaluate precision using homography validation
# 4. Save results to results/ directory
```

#### Dataset Requirements
The program expects the HPatches dataset in the `data/` directory:
```
data/
â”œâ”€â”€ i_ajuntament/
â”‚   â”œâ”€â”€ 1.ppm
â”‚   â”œâ”€â”€ 2.ppm
â”‚   â”œâ”€â”€ ...
â”‚   â”œâ”€â”€ H_1_2
â”‚   â”œâ”€â”€ H_1_3
â”‚   â””â”€â”€ ...
â””â”€â”€ (other scene folders)
```

**Download dataset automatically:**
```bash
python3 setup.py
```

#### YAML Configuration Format

Create experiment configurations in `config/experiments/`. Example:

```yaml
experiment:
  name: "my_sift_experiment"
  description: "Custom SIFT experiment"

dataset:
  path: "../data/"

descriptors:
  - name: "sift_baseline"
    type: "sift"
    pooling: "none"
  - name: "sift_with_dsp"
    type: "sift"
    pooling: "domain_size_pooling"
    # Optional DSP parameters
    scales: [0.75, 1.0, 1.25]           # domain size multipliers (keypoint-size scaling)
    # Choose one of the following weighting modes:
    # 1) Explicit weights aligned with scales
    # scale_weights: [1.0, 2.0, 1.0]
    # 2) Procedural weighting
    scale_weighting: gaussian            # gaussian|triangular|uniform
    scale_weight_sigma: 0.15            # sigma in log-space (triangular uses as radius proxy)
    # Normalization/RootSIFT stages (row-wise)
    # normalize_before_pooling: false
    # normalize_after_pooling: true
    # rooting options: R_BEFORE_POOLING or R_AFTER_POOLING (via legacy or CLI fields)

evaluation:
  matching:
    method: "brute_force"
    threshold: 0.8
  validation:
    method: "homography"
    threshold: 0.05

output:
  results_path: "results/"
  save_visualizations: true

database:
  enabled: false
```

**Available descriptor types:** `sift`, `rgbsift`, `vsift`, `honc`  
**Available pooling strategies:** `none`, `domain_size_pooling`, `stacking`

DSP-SIFT (Domain Size Pooling):
- Uses keypoint-size scaling over `scales` on the original image (no image resizing).
- Pools per-scale descriptors by weighted average: explicit `scale_weights` or procedural `scale_weighting` + `scale_weight_sigma`.
- Normalizes per row (L2) by default after pooling; RootSIFT supported (L1 then sqrt) before/after.

#### Legacy Configuration Options

**Descriptor Types:**
- `DESCRIPTOR_SIFT`: Standard SIFT
- `DESCRIPTOR_vSIFT`: Vanilla SIFT implementation
- `DESCRIPTOR_RGBSIFT`: RGB color SIFT
- `DESCRIPTOR_HoNC`: Histogram of Normalized Colors

**Processing Options:**
- `NONE`: No additional processing
- `DOMAIN_SIZE_POOLING`: Multi-scale descriptor pooling
- `STACKING`: Descriptor combination

**Modify configuration in:**
- `descriptor_compare/main.cpp` - Main experiment loop
- `descriptor_compare/experiment_config.hpp` - Configuration options

### Expected Output

```
Processing folder: /path/to/data/i_ajuntament
Results folder: /path/to/results/experiment_name/i_ajuntament
Hostname: container_id (if running in Docker)
Processing locked folder: /path/to/data/v_wall
Results folder: /path/to/results/experiment_name/v_wall
...
```

**CLI Results saved to:**
- `results/experiment_name/descriptor_name/scene_name/results.csv`
- Example: `results/sift_baseline/sift/i_dome/results.csv`
- Contains precision measurements for each image pair

**Legacy Results saved to:**
- `results/experiment_name/scene_name/results.csv`
- Contains precision measurements for each image pair

## Database Integration

The project now features a complete database-driven workflow for experiment tracking and keypoint management.

### Database Features

âœ… **Real-Time Experiment Tracking**: All experiments automatically tracked in SQLite database  
âœ… **Comprehensive Metrics Storage**: Precision, recall, MAP, processing time, metadata  
âœ… **Keypoint Management**: Database storage for reproducible locked-in keypoints  
âœ… **CLI Tools**: Command-line tools for database and keypoint operations  
âœ… **No More CSV Dependencies**: Database-first workflow eliminates CSV file dependencies  

### Database Setup

The database is **automatically enabled** with the default build:

```bash
# Database is enabled by default
cmake .. -DBUILD_DATABASE=ON  # (default)
make -j$(nproc)

# Database file created at: build/experiments.db
```

### Using the Database

#### Viewing Database Contents

```bash
cd build

# SQLite CLI
sqlite3 experiments.db
.schema                         # View table structure
SELECT * FROM experiments;      # View experiment configs  
SELECT * FROM results;          # View experiment results
SELECT * FROM locked_keypoints; # View stored keypoints

# Or use any SQLite GUI tool
```

#### Database Schema

**experiments** table:
- `id`, `descriptor_type`, `dataset_name`, `pooling_strategy`
- `similarity_threshold`, `max_features`, `timestamp`, `parameters`

**results** table:
- `experiment_id`, `mean_average_precision`, `precision_at_1`, `precision_at_5`
- `recall_at_1`, `recall_at_5`, `total_matches`, `total_keypoints`
- `processing_time_ms`, `timestamp`, `metadata`

**locked_keypoints** table:
- `scene_name`, `image_name`, `x`, `y`, `size`, `angle`
- `response`, `octave`, `class_id`, `created_at`

### Keypoint Management System

The project includes a complete CLI tool for managing locked-in keypoints in the database.

#### Keypoint Manager Commands

```bash
cd build

# Generate fresh keypoints from images and store in database
./keypoint_manager generate ../data

# Import keypoints from existing CSV files for reproducibility  
./keypoint_manager import-csv ../reference_keypoints

# Export keypoints from database to CSV for long-term storage
./keypoint_manager export-csv ./exported_keypoints

# List all scenes with keypoint counts
./keypoint_manager list-scenes

# Count keypoints for specific scene/image
./keypoint_manager count i_dome 1.ppm
```

#### Keypoint Workflow Examples

**Generate Fresh Keypoints:**
```bash
# Clear database and generate new keypoints from all images
./keypoint_manager generate ../data

# Output:
# ğŸ”„ Generating fresh locked keypoints from: ../data
# ğŸ—‘ï¸  Clearing existing keypoints from database...
# ğŸ“ Processing scene: i_dome
# âœ… Generated 6573 keypoints for i_dome/1.ppm
# âœ… Stored 6573 keypoints for i_dome/2.ppm
# ğŸ‰ Generation complete! Total keypoints generated: 78876
```

**Import from CSV for Reproducibility:**
```bash
# Import pre-computed keypoints from reference files
./keypoint_manager import-csv ../reference_keypoints

# Perfect for:
# - Sharing reproducible experiments
# - Comparing against published results  
# - Starting from known keypoint sets
```

**Export for Long-Term Storage:**
```bash
# Export current database keypoints to CSV files
./keypoint_manager export-csv ./backup_keypoints

# Creates organized CSV structure:
# backup_keypoints/
# â”œâ”€â”€ i_dome/
# â”‚   â”œâ”€â”€ 1ppm.csv
# â”‚   â”œâ”€â”€ 2ppm.csv
# â”‚   â””â”€â”€ ...
# â””â”€â”€ v_wall/
#     â”œâ”€â”€ 1ppm.csv
#     â””â”€â”€ ...
```

**Query Keypoint Status:**
```bash
# See what's in the database
./keypoint_manager list-scenes

# Output:
# ğŸ“‹ Available scenes (2):
#   ğŸ“ i_dome (6 images, 39438 total keypoints)
#   ğŸ“ v_wall (6 images, 41259 total keypoints)

# Get specific counts
./keypoint_manager count i_dome 1.ppm
# ğŸ”¢ Keypoints for i_dome/1.ppm: 6573
```

### Database vs CSV Workflow

**OLD Workflow (CSV-dependent):**
```bash
# 1. Generate CSV files manually
# 2. Store keypoints in reference_keypoints/
# 3. Load from CSV during experiments  
# 4. Results saved to CSV files
# 5. Analyze CSV files separately
```

**NEW Workflow (Database-first):**
```bash
# 1. Generate keypoints directly to database
./keypoint_manager generate ../data

# 2. Run experiments with real-time database tracking
./experiment_runner ../config/experiments/sift_baseline.yaml

# 3. Query results directly from database
sqlite3 experiments.db "SELECT mean_average_precision, processing_time_ms FROM results;"

# 4. Export only when needed for sharing
./keypoint_manager export-csv ./published_keypoints
```

### Migration from CSV to Database

If you have existing CSV keypoints:

```bash
# 1. Import your existing CSV keypoints  
./keypoint_manager import-csv ../reference_keypoints

# 2. Verify import
./keypoint_manager list-scenes

# 3. Run experiments (now uses database)
./experiment_runner ../config/experiments/sift_baseline.yaml

# 4. Your CSV files are preserved for compatibility
```

### Troubleshooting Database Issues

#### Database File Not Found
```bash
# Database file is created automatically in build directory
ls build/experiments.db

# If missing, run any experiment to create it
cd build && ./descriptor_compare
```

#### Permission Issues
```bash
# Fix database permissions
chmod 644 build/experiments.db
```

#### View Database Statistics
```bash
# Check experiment history
sqlite3 build/experiments.db "
SELECT 
    e.descriptor_type,
    r.mean_average_precision,
    r.processing_time_ms,
    r.timestamp
FROM experiments e 
JOIN results r ON e.id = r.experiment_id 
ORDER BY r.timestamp DESC 
LIMIT 10;"
```

## Project Structure

```
descriptor-compare/
â”œâ”€â”€ CMakeLists.txt              # Build configuration
â”œâ”€â”€ Dockerfile                  # Docker environment
â”œâ”€â”€ docker-compose.dev.yml     # Development container config
â”œâ”€â”€ setup.py                    # Dataset download script
â”œâ”€â”€ environment.yml             # Conda environment (optional)
â”œâ”€â”€ conanfile.txt              # Conan dependencies (optional)
â”‚
â”œâ”€â”€ cli/                        # NEW: Command-line tools
â”‚   â”œâ”€â”€ experiment_runner.cpp   # YAML-based experiment runner
â”‚   â”œâ”€â”€ keypoint_manager.cpp    # Database keypoint management
â”‚   â””â”€â”€ analysis_runner.cpp     # Analysis pipeline runner
â”‚
â”œâ”€â”€ config/                     # NEW: YAML experiment configurations
â”‚   â””â”€â”€ experiments/
â”‚       â”œâ”€â”€ sift_baseline.yaml
â”‚       â”œâ”€â”€ rgbsift_comparison.yaml
â”‚       â””â”€â”€ sample_experiment.yaml
â”‚
â”œâ”€â”€ keypoints/                  # Descriptor implementations
â”‚   â”œâ”€â”€ VanillaSIFT.h/cpp      # Base SIFT implementation
â”‚   â”œâ”€â”€ DSPSIFT.h/cpp          # Domain Size Pooling SIFT
â”‚   â”œâ”€â”€ RGBSIFT.h/cpp          # RGB Color SIFT
â”‚   â”œâ”€â”€ HoNC.h/cpp             # Histogram of Normalized Colors
â”‚   â””â”€â”€ HoWH.h/cpp             # Hue-weighted descriptor
â”‚
â”œâ”€â”€ descriptor_compare/         # Main application
â”‚   â”œâ”€â”€ main.cpp               # Main experiment runner
â”‚   â”œâ”€â”€ experiment_config.hpp  # Configuration management
â”‚   â”œâ”€â”€ image_processor.hpp    # Image processing pipeline
â”‚   â”œâ”€â”€ processor_utils.hpp    # Utility functions
â”‚   â””â”€â”€ locked_in_keypoints.hpp # Keypoint management
â”‚
â”œâ”€â”€ database/                  # NEW: Database integration
â”‚   â”œâ”€â”€ schema.sql             # Database schema definition
â”‚   â””â”€â”€ database_manager.cpp   # Legacy database implementation
â”‚
â”œâ”€â”€ src/core/database/         # NEW: Modern database system
â”‚   â””â”€â”€ DatabaseManager.cpp    # SQLite integration & keypoint storage
â”‚
â”œâ”€â”€ data/                      # HPatches dataset (created by setup.py)
â”œâ”€â”€ results/                   # Experiment results
â”œâ”€â”€ reference_keypoints/       # Pre-computed keypoints (CSV)
â””â”€â”€ build/experiments.db       # SQLite database (auto-created)
```
Current goal state

``` 
 descriptor-research/
  â”œâ”€â”€ CMakeLists.txt                 # Build system
  â”œâ”€â”€ setup.py                      # Dataset downloader
  â”œâ”€â”€ docker-compose.dev.yml        # Development environment
  â”‚
  â”œâ”€â”€ cli/                          # âœ… ACTIVE: Database-first CLI tools
  â”‚   â”œâ”€â”€ experiment_runner.cpp     # YAML experiment runner (database-tracked)
  â”‚   â”œâ”€â”€ keypoint_manager.cpp      # Database keypoint CRUD operations
  â”‚   â””â”€â”€ analysis_runner.cpp       # Analysis pipeline
  â”‚
  â”œâ”€â”€ config/                       # âœ… ACTIVE: YAML experiment configs
  â”‚   â””â”€â”€ experiments/
  â”‚       â”œâ”€â”€ sift_baseline.yaml
  â”‚       â”œâ”€â”€ rgbsift_comparison.yaml
  â”‚       â””â”€â”€ honc_comparison.yaml
  â”‚
  â”œâ”€â”€ descriptor_compare/           # âœ… ACTIVE: Main application (database-integrated)
  â”‚   â”œâ”€â”€ main.cpp                  # Database-tracked experiments
  â”‚   â”œâ”€â”€ image_processor.cpp       # DATABASE KEYPOINT LOADING (not CSV!)
  â”‚   â”œâ”€â”€ experiment_config.hpp     # Legacy descriptor creation
  â”‚   â””â”€â”€ processor_utils.cpp       # Legacy processing pipeline
  â”‚
  â”œâ”€â”€ keypoints/                    # âœ… ACTIVE: Legacy descriptor implementations
  â”‚   â”œâ”€â”€ VanillaSIFT.cpp          # Working implementations
  â”‚   â”œâ”€â”€ RGBSIFT.cpp              # Currently used by main app
  â”‚   â”œâ”€â”€ HoNC.cpp                 # Working implementations
  â”‚   â””â”€â”€ ...
  â”‚
  â”œâ”€â”€ src/core/                     # ğŸš§ FUTURE: Modern architecture (Stage 8+)
  â”‚   â”œâ”€â”€ database/
  â”‚   â”‚   â””â”€â”€ DatabaseManager.cpp   # âœ… ACTIVE: Currently used
  â”‚   â”œâ”€â”€ descriptor/               # ğŸš§ DORMANT: Built but unused
  â”‚   â”‚   â”œâ”€â”€ DescriptorFactory.cpp # Future migration target
  â”‚   â”‚   â””â”€â”€ extractors/wrappers/  # Future implementations
  â”‚   â””â”€â”€ integration/
  â”‚       â””â”€â”€ ProcessorBridge.cpp   # ğŸš§ DORMANT: Built but unused
  â”‚
  â”œâ”€â”€ database/                     # âœ… ACTIVE: Database system
  â”‚   â”œâ”€â”€ schema.sql               # Current schema definition
  â”‚   â””â”€â”€ experiments.db           # Runtime database (auto-created)
  â”‚
  â”œâ”€â”€ data/                        # âœ… ACTIVE: HPatches dataset
  â”œâ”€â”€ results/                     # âœ… ACTIVE: Experiment outputs
  â”œâ”€â”€ reference_keypoints/         # âŒ DEPRECATED: CSV compatibility only
  â””â”€â”€ build/experiments.db         # âœ… ACTIVE: Main database file

```

## Troubleshooting

### Common Build Issues

#### OpenCV SIFT Not Found
```bash
# Verify SIFT support
python3 -c "import cv2; print('SIFT available:', hasattr(cv2, 'SIFT_create'))"

# If false, install OpenCV contrib:
# Manjaro: yay -S opencv-contrib
# Ubuntu: sudo apt install libopencv-contrib-dev
```

#### CMake Cache Issues
```bash
# Clean build directory
rm -rf build/
mkdir build && cd build
cmake .. && make
```

#### Docker Permission Issues
```bash
# Ensure user is in docker group
sudo usermod -aG docker $USER
# Log out and back in

# Fix file permissions
sudo chown -R $USER:$USER .
```

### Docker-Specific Issues

#### Container Won't Start
```bash
# Check Docker service
sudo systemctl status docker

# Rebuild image
docker-compose -f docker-compose.dev.yml build --no-cache
```

#### X11 Forwarding Issues
```bash
# Allow X11 connections
xhost +local:docker

# Test X11 in container
docker-compose -f docker-compose.dev.yml run descriptor-dev python3 -c "
import cv2
import numpy as np
img = np.zeros((100,100,3), dtype=np.uint8)
cv2.imshow('Test', img)
cv2.waitKey(1000)
"
```

#### Build Path Conflicts
```bash
# Use separate build directories
mkdir build-native    # for native builds
mkdir build-docker    # for Docker builds
```

### Dataset Issues

#### Dataset Not Found
```bash
# Verify dataset downloaded
ls data/
# Should show scene folders like i_ajuntament, v_wall, etc.

# Re-download if needed
python3 setup.py
```

#### Permission Issues
```bash
# Fix data directory permissions
sudo chown -R $USER:$USER data/
```

### Windows-Specific Issues

#### Docker Desktop Requirements
```powershell
# Ensure WSL2 is enabled
wsl --install
# Restart computer after WSL2 installation

# Enable virtualization in BIOS if needed
# Docker Desktop â†’ Settings â†’ General â†’ Use WSL2 based engine
```

#### Visual Studio Build Issues
```powershell
# Ensure correct Visual Studio components installed:
# - MSVC v143 compiler toolset
# - Windows 10/11 SDK
# - CMake tools for C++

# Set environment variables if using vcpkg
$env:VCPKG_ROOT = "C:\path\to\vcpkg"
$env:CMAKE_TOOLCHAIN_FILE = "$env:VCPKG_ROOT\scripts\buildsystems\vcpkg.cmake"
```

#### Path Length Issues
```powershell
# Enable long path support (Windows 10+)
# Run as Administrator:
New-ItemProperty -Path "HKLM:\SYSTEM\CurrentControlSet\Control\FileSystem" -Name "LongPathsEnabled" -Value 1 -PropertyType DWORD -Force
```

#### PowerShell Execution Policy
```powershell
# Allow script execution
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

## Development Guidelines

### Adding New Descriptors
1. **Create descriptor class** inheriting from appropriate base
2. **Add to DescriptorType enum** in `experiment_config.hpp`
3. **Register in createDescriptorExtractor()** factory method
4. **Test with both native and Docker builds**

### Contributing
1. **Test changes** in both native and Docker environments
2. **Ensure reproducible results** across environments
3. **Update documentation** for new features
4. **Follow existing code structure** and naming conventions

### Performance Testing
- **Use Release builds** for performance measurements
- **Test in Docker** to ensure reproducible timing
- **Profile with tools** like gdb, valgrind (available in Docker)

## Quick Start Workflow

### 1. Setup
```bash
git clone <your-repository-url>
cd descriptor-compare
python3 setup.py                    # Download dataset
mkdir build && cd build
cmake .. -DUSE_SYSTEM_PACKAGES=ON -DBUILD_DATABASE=ON
make -j$(nproc)
```

### 2. Setup Keypoints (Database-First)
```bash
# Generate keypoints directly to database (recommended)
./keypoint_manager generate ../data

# OR import existing CSV keypoints
./keypoint_manager import-csv ../reference_keypoints

# Verify keypoints loaded
./keypoint_manager list-scenes
```

### 3. Run Experiments (Database-Tracked)
```bash
# Run individual experiments (automatically tracked in database)
./experiment_runner ../config/experiments/sift_baseline.yaml
./experiment_runner ../config/experiments/rgbsift_comparison.yaml

# Check results in database
sqlite3 experiments.db "SELECT * FROM results ORDER BY timestamp DESC LIMIT 5;"

# OR check traditional CSV results
ls results/sift_baseline/sift/
cat results/sift_baseline/sift/i_dome/results.csv
```

### 4. Create Custom Experiments
```bash
# Copy existing config
cp ../config/experiments/sift_baseline.yaml ../config/experiments/my_experiment.yaml

# Edit your config
# Change experiment name, descriptors, parameters

# Run your experiment (results automatically tracked)
./experiment_runner ../config/experiments/my_experiment.yaml
```

### 5. Query and Analyze Results
```bash
# Query database for experiment comparison
sqlite3 experiments.db "
SELECT 
    e.descriptor_type,
    r.mean_average_precision,
    r.processing_time_ms 
FROM experiments e 
JOIN results r ON e.id = r.experiment_id 
ORDER BY r.mean_average_precision DESC;"

# Run analysis pipeline
./analysis_runner results/ --full

# View reports
open analysis/outputs/analysis_report.html
```

### 6. Export Results for Sharing
```bash
# Export keypoints for reproducibility
./keypoint_manager export-csv ./published_keypoints

# Share database file
cp experiments.db ./my_research_results.db
```

## Support

For issues, feature requests, or questions:
1. **Check troubleshooting section** above
2. **Verify environment setup** (native vs Docker)
3. **Test with minimal example** first
4. **Report with environment details** (OS, Docker version, etc.)

---

**Note**: This project requires the HPatches dataset for meaningful experiments. The `setup.py` script will automatically download and organize the dataset in the correct structure.
